// EmailFromStartConsumer.java
package workshop;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.util.UUID;

        import org.apache.kafka.clients.consumer.ConsumerRecord;
        import org.apache.kafka.clients.consumer.ConsumerRecords;
        import org.apache.kafka.clients.consumer.KafkaConsumer;

        import java.util.Properties;

        import static java.time.Duration.ofSeconds;
        import static java.util.Collections.singletonList;
        import static org.apache.kafka.clients.consumer.ConsumerConfig.*;

        import org.apache.kafka.clients.consumer.ConsumerRecord;
        import org.apache.kafka.clients.consumer.ConsumerRecords;
        import org.apache.kafka.clients.consumer.KafkaConsumer;

        import java.util.Properties;
        import java.util.UUID;

        import static java.time.Duration.ofSeconds;
        import static org.apache.kafka.clients.consumer.ConsumerConfig.*;
        import static java.util.Collections.singletonList;

// Read data from offset 0, all time
public class EmailFromStartConsumer {

    public static String BOOTSTRAP_SERVERS = "localhost:9092";
    public static String TOPIC = "emails";

    public static void main(String[] args) {

        Properties props = new Properties();
        props.put(BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);

        // RANDOM Consumer group ID
        props.put(GROUP_ID_CONFIG, UUID.randomUUID().toString());
        props.put(AUTO_OFFSET_RESET_CONFIG, "earliest");

        props.put(ENABLE_AUTO_COMMIT_CONFIG, "false");
        props.put(AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        props.put(SESSION_TIMEOUT_MS_CONFIG, "30000");
        props.put(KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");

        // <Key as string, Value as string>
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        // Subscribe for topic(s)
        consumer.subscribe(singletonList(TOPIC));

        System.out.println("Consumer Starting!");

        while (true) {
            // poll (timeout value), wait for 1 second, get all the messages
            // <Key, Value>
            ConsumerRecords<String, String> records = consumer.poll(ofSeconds(1));
            // if no messages
            if (records.count() == 0)
                continue;

            // Iterating over each record
            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("partition= %d, offset= %d, key= %s, value= %s\n",
                        record.partition(),
                        record.offset(),
                        record.key(),
                        record.value());
            }

            // send ack to broker, msg are processed
            consumer.commitSync();
        }
    }
}